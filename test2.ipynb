{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\shash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load ClinicalBERT model and tokenizer\n",
    "model_name = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing tokens: 100%|██████████| 28996/28996 [16:04<00:00, 30.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Get the list of tokens in the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Prepare a dictionary to store token embeddings\n",
    "token_embeddings = {}\n",
    "\n",
    "# Iterate over the vocabulary to get embeddings for each token\n",
    "for token, token_id in tqdm(vocab.items(), desc=\"Processing tokens\", total=len(vocab)):\n",
    "    # Tokenize and get input IDs\n",
    "    inputs = tokenizer(token, return_tensors='pt')\n",
    "    \n",
    "    # Get hidden states (embeddings) from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embedding for the token (taking the first token's embedding)\n",
    "    embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "    \n",
    "    # Store the token and its corresponding embedding\n",
    "    token_embeddings[token] = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to clinicalbert_embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the embeddings to a txt file\n",
    "embedding_file = 'clinicalbert_embeddings.txt'\n",
    "\n",
    "with open(embedding_file, 'w', encoding='utf-8') as f:\n",
    "    for token, embedding in token_embeddings.items():\n",
    "        embedding_str = ' '.join(map(str, embedding))\n",
    "        f.write(f\"{token} {embedding_str}\\n\")\n",
    "\n",
    "print(f\"Embeddings saved to {embedding_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load BlueBERT model and tokenizer\n",
    "# Load model directly\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Embeddings: 100%|██████████| 28996/28996 [6:19:50<00:00,  1.27it/s]      \n"
     ]
    }
   ],
   "source": [
    "# Get the list of tokens in the vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Prepare a dictionary to store token embeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Prepare a dictionary to store token embeddings\n",
    "token_embeddings = {}\n",
    "\n",
    "# Iterate over the vocabulary to get embeddings for each token\n",
    "for token, token_id in tqdm(vocab.items(), desc=\"Extracting Embeddings\"):\n",
    "    # Tokenize and get input IDs\n",
    "    inputs = tokenizer(token, return_tensors='pt')\n",
    "    \n",
    "    # Get hidden states (embeddings) from the model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the embedding for the token (taking the first token's embedding)\n",
    "    embedding = outputs.last_hidden_state[0][0].numpy()\n",
    "    \n",
    "    # Store the token and its corresponding embedding\n",
    "    token_embeddings[token] = embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to bluebert_embeddings.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the embeddings to a txt file\n",
    "embedding_file = 'bluebert_embeddings.txt'\n",
    "\n",
    "with open(embedding_file, 'w', encoding='utf-8') as f:\n",
    "    for token, embedding in token_embeddings.items():\n",
    "        embedding_str = ' '.join(map(str, embedding))\n",
    "        f.write(f\"{token} {embedding_str}\\n\")\n",
    "\n",
    "print(f\"Embeddings saved to {embedding_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
